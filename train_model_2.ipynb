{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_model_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zuIm2Q3EOBJ",
        "colab_type": "code",
        "outputId": "9a346273-e02f-4968-c2c6-acb6d85626a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "print(f'PyTorch version: {torch.__version__}')\n",
        "\n",
        "if torch.cuda.is_available()==True:\n",
        "    use_cuda = True\n",
        "    print(f'GPU available: {torch.cuda.get_device_name(0)} ({torch.cuda.device_count()} count)')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version: 1.3.1+cu100\n",
            "GPU available: Tesla K80 (1 count)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45E6tB-cFq0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def default_device():\n",
        "    if torch.cuda.is_available()==True:\n",
        "        dflt_device = torch.device('cuda')\n",
        "    else:\n",
        "        dflt_device = torch.device('cpu')\n",
        "\n",
        "    return dflt_device\n",
        "\n",
        "\n",
        "def load_data(dir_in_str):\n",
        "    E, S_pre, durations_list, min_pitch, max_pitch = torch.load(dir_in_str)\n",
        "    E = E.to(device=dflt_device)\n",
        "    S = []\n",
        "    for tensor in S_pre:\n",
        "        S.append(tensor.to(device=dflt_device))\n",
        "\n",
        "    return E, S, durations_list, min_pitch, max_pitch\n",
        "\n",
        "\n",
        "def dimensions(E,S): \n",
        "    \n",
        "    num_event_examples, num_events , event_emb_size  = E.shape    \n",
        "    num_seq_examples = len(S)\n",
        "    signal_emb_size = S[0].size(1)\n",
        "    \n",
        "    dims = [num_event_examples, num_events , event_emb_size, num_seq_examples, signal_emb_size ]\n",
        "    \n",
        "    return dims\n",
        "  \n",
        "\n",
        "def prepare_data(S):\n",
        "  S_pre_input = []\n",
        "  first_row = torch.zeros(1,signal_emb_size).to(device=dflt_device)\n",
        "  for tensor in S:\n",
        "      expanded_tensor = torch.cat((first_row, tensor), dim=0)\n",
        "      new_tensor = expanded_tensor[:-1,:]\n",
        "      S_pre_input.append(new_tensor)\n",
        "      \n",
        "  conditioning_idxs_vectors = [] \n",
        "  for tensor in S:\n",
        "      conditioning_indices = torch.zeros(tensor.shape[0],1).to(device=dflt_device)\n",
        "      cumulative_duration = 0\n",
        "      for row in range(0,tensor.shape[0]-1):\n",
        "          vector = tensor[row,:]        \n",
        "          pitch_idx, rhythm_idx = list((vector != 0).nonzero())\n",
        "          pitch_idx, rhythm_idx = int(pitch_idx), int(rhythm_idx)\n",
        "          duration_type_idx = rhythm_idx - rhythm_idx_ini\n",
        "          duration_type = durations_list[duration_type_idx]\n",
        "          cumulative_duration += duration_type\n",
        "          conditioning_indices[row+1] = int(cumulative_duration)\n",
        "      conditioning_idxs_vectors.append(conditioning_indices)\n",
        "\n",
        "  lengths_list = []\n",
        "  for tensor in S:\n",
        "      lengths_list.append(tensor.shape[0])\n",
        "\n",
        "  S_padded = torch.nn.utils.rnn.pad_sequence(S, batch_first=True)\n",
        "  S_packed = torch.nn.utils.rnn.pack_padded_sequence(S_padded, batch_first=True, lengths=lengths_list, enforce_sorted=False)\n",
        "\n",
        "  return S_packed, S_padded, S_pre_input, lengths_list, conditioning_idxs_vectors\n",
        "\n",
        "\n",
        "def create_placing_matrices(conditioning_idxs_vectors, num_events):\n",
        "    placing_conditioning_matrices = []\n",
        "    for vector in conditioning_idxs_vectors:\n",
        "        placing_matrix = torch.zeros(vector.shape[0], num_events).to(device=dflt_device)\n",
        "        for i in range(vector.shape[0]):\n",
        "            placing_matrix[i, int(vector[i])] = 1\n",
        "        placing_conditioning_matrices.append(placing_matrix)\n",
        "        \n",
        "    return placing_conditioning_matrices\n",
        "\n",
        "\n",
        "def concatenate_conditioning(S_pre_input, \\\n",
        "                             encoded_conditioning, \\\n",
        "                             placing_conditioning_matrices, lengths_list):    \n",
        "  S_conditioned = []\n",
        "  for idx, tensor in enumerate(S_pre_input):\n",
        "    placing_matrix = placing_conditioning_matrices[idx]\n",
        "    dynamic_conditioning = torch.mm(placing_matrix, encoded_conditioning[idx,:,:])\n",
        "    concatenated_input = torch.cat((tensor, dynamic_conditioning), dim=1)\n",
        "    S_conditioned.append(concatenated_input)\n",
        "  S_input = torch.nn.utils.rnn.pad_sequence(S_conditioned, batch_first=True)\n",
        "  S_input = torch.nn.utils.rnn.pack_padded_sequence(S_input, batch_first=True, lengths=lengths_list, enforce_sorted=False)\n",
        "\n",
        "  return S_input\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_avKGXDBKxce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class event_net(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, event_emb_size, event_hidden_size, event_output_size, \\\n",
        "                 num_event_layers, num_event_examples, num_directions):\n",
        "        super(event_net, self).__init__()\n",
        "\n",
        "        self.event_emb_size     = event_emb_size\n",
        "        self.event_hidden_size  = event_hidden_size\n",
        "        self.event_output_size  = event_output_size\n",
        "        self.num_event_layers   = num_event_layers\n",
        "        self.num_event_examples = num_event_examples\n",
        "        self.num_directions     = num_directions\n",
        "        \n",
        "        self.event_lstm   = torch.nn.LSTM(self.event_emb_size, self.event_hidden_size, \\\n",
        "                                    self.num_event_layers, batch_first=True, bidirectional=True)\n",
        "        self.event_linear = torch.nn.Linear(self.event_hidden_size*num_directions, self.event_output_size)\n",
        "        \n",
        "        self.initHidden  = self.init_hidden()\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        h_ini = (torch.zeros(self.num_event_layers*num_directions, self.num_event_examples, self.event_hidden_size),\\\n",
        "              torch.zeros(self.num_event_layers*num_directions, self.num_event_examples, self.event_hidden_size) )\n",
        "          \n",
        "    def forward(self, Events):\n",
        "        event_lstm_out, event_hidden = self.event_lstm(Events, self.initHidden)\n",
        "        linear_output = self.event_linear(event_lstm_out*num_event_layers)\n",
        "        event_output = torch.sigmoid(linear_output)\n",
        "        \n",
        "        return event_output\n",
        "\n",
        "\n",
        "class signal_net(torch.nn.Module):\n",
        "    def __init__(self, signal_emb_size, conditioning_size, signal_hidden_size, \\\n",
        "                 signal_output_size, num_signal_layers, num_signal_examples):\n",
        "        super(signal_net, self).__init__()\n",
        "        \n",
        "        self.signal_emb_size     = signal_emb_size\n",
        "        self.conditioning_size   = conditioning_size\n",
        "        self.signal_hidden_size  = signal_hidden_size\n",
        "        self.signal_output_size  = signal_output_size\n",
        "        self.num_signal_layers   = num_signal_layers\n",
        "        self.num_signal_examples = num_signal_examples\n",
        "        \n",
        "        self.signal_lstm   = torch.nn.LSTM(self.signal_emb_size+self.conditioning_size, self.signal_hidden_size, \\\n",
        "                                    self.num_signal_layers, batch_first=True)\n",
        "        self.signal_linear = torch.nn.Linear(self.signal_hidden_size, self.signal_output_size)\n",
        "        \n",
        "    def forward(self, S_input, prev_hidden):\n",
        "        signal_lstm_out, signal_hidden = self.signal_lstm(S_input, prev_hidden)\n",
        "        signal_linear_output = self.signal_linear(signal_lstm_out.data)\n",
        "        #signal_output = torch.sigmoid(signal_linear_output)\n",
        "        \n",
        "        return signal_linear_output, signal_hidden\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_Mw05tKHE7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dflt_device = default_device()\n",
        "\n",
        "E, S, durations_list, min_pitch, max_pitch = load_data('Parker_Dataset_unshuffled.pt')\n",
        "#E, S = E[0:6,:,:], S[0:6]\n",
        "\n",
        "num_event_examples, num_events , event_emb_size, num_signal_examples, signal_emb_size = dimensions(E,S)\n",
        "rhythm_idx_ini = max_pitch - min_pitch + 1 + True\n",
        "\n",
        "S_packed, S_padded, S_pre_input, lengths_list, conditioning_idxs_vectors = prepare_data(S)\n",
        "placing_conditioning_matrices = create_placing_matrices(conditioning_idxs_vectors, num_events)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F4G6u8YVYEf",
        "colab_type": "code",
        "outputId": "442fbcaa-738d-4bc4-aa05-18b0790d4a62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "#Choose dimensions for event LSTM\n",
        "num_event_layers = 1\n",
        "event_hidden_size = 32\n",
        "num_directions = 2\n",
        "event_output_size = 48\n",
        "\n",
        "#Choose dimensions for signal LSTM\n",
        "num_signal_layers  = 1\n",
        "signal_hidden_size = 128\n",
        "signal_output_size = 89\n",
        "conditioning_size  = event_output_size\n",
        "\n",
        "#Create 1st LSTM\n",
        "event_forward_pass = event_net(event_emb_size, event_hidden_size, event_output_size, \\\n",
        "                 num_event_layers, num_event_examples, num_directions)\n",
        "event_forward_pass = event_forward_pass.to(device=dflt_device)\n",
        "\n",
        "#Create 2nd LSTM\n",
        "signal_forward_pass = signal_net(signal_emb_size, conditioning_size, signal_hidden_size, \\\n",
        "                 signal_output_size, num_signal_layers, num_signal_examples)\n",
        "signal_forward_pass = signal_forward_pass.to(device=dflt_device)\n",
        "signal_h_ini = (torch.zeros(num_signal_layers, num_signal_examples, signal_hidden_size).to(device=dflt_device),\\\n",
        "              torch.zeros(num_signal_layers, num_signal_examples, signal_hidden_size).to(device=dflt_device) )\n",
        "\n",
        "weights = list(event_forward_pass.parameters()) + list(signal_forward_pass.parameters())\n",
        "\n",
        "#Number of parameters\n",
        "num_event_parameters = sum([p.numel() for p in event_forward_pass.parameters()])\n",
        "print(f'Number of parameters in LSTM of events: {num_event_parameters}')\n",
        "\n",
        "num_signal_parameters = sum([p.numel() for p in signal_forward_pass.parameters()])\n",
        "print(f'Number of parameters in LSTM of signals: {num_signal_parameters}')\n",
        "\n",
        "print(f'Total number of parameters: {num_event_parameters+num_signal_parameters}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters in LSTM of events: 17968\n",
            "Number of parameters in LSTM of signals: 148185\n",
            "Total number of parameters: 166153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6PqpNP33dok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR          = 0.05\n",
        "epochs      = 3000\n",
        "WeightDecay = 1e-8\n",
        "Momentum    = 0.9\n",
        "\n",
        "loss_func    = torch.nn.BCEWithLogitsLoss()\n",
        "#loss_func    = torch.nn.MSELoss()\n",
        "optimizer    = torch.optim.Adam(weights, lr=LR, betas=(0.9, 0.999), eps=1e-8, weight_decay = WeightDecay )\n",
        "#optimizer    = torch.optim.RMSprop(weights,lr=LR, alpha=0.99, eps=1e-8, weight_decay = WeightDecay, momentum = Momentum, centered=True)\n",
        "scheduler    = torch.optim.lr_scheduler.StepLR(optimizer, step_size=300, gamma=0.5, last_epoch=-1)\n",
        "#scheduler    = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0.000001, last_epoch=-1)\n",
        "\n",
        "loss_hist  = []\n",
        "for epoch in range(1, epochs+1):\n",
        "  t = time.time()\n",
        "  optimizer.zero_grad()\n",
        "  encoded_conditioning = event_forward_pass(E)\n",
        "  S_input = concatenate_conditioning(S_pre_input, encoded_conditioning, placing_conditioning_matrices, lengths_list)\n",
        "  S_hat, _ = signal_forward_pass(S_input, signal_h_ini)\n",
        "  Loss = loss_func(S_hat, S_packed.data)\n",
        "  Loss.backward()\n",
        "  optimizer.step()\n",
        "  loss_hist.append(Loss.item())\n",
        "  scheduler.step()\n",
        "  #if epoch%200==0:\n",
        "  print(f'Epoch: {epoch}, Loss: {Loss}  (Learning rate: {scheduler.get_lr()}, Time: {round(time.time()-t,4)}s')\n",
        "\n",
        "plt.plot(loss_hist[:])\n",
        "plt.xlabel('Gradient Steps')\n",
        "vert_label=plt.ylabel('Loss')\n",
        "vert_label.set_rotation(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNCtOASxL2Bq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def net_generate(e, sample=False):\n",
        "  event_steps = e.shape[0]\n",
        "  encoded_conditioning = event_forward_pass(e.view(1, event_steps, -1))\n",
        "  hidden = (signal_h_ini[0][0,0,:].view(num_signal_layers, 1, signal_hidden_size), \\\n",
        "           signal_h_ini[1][0,0,:].view(num_signal_layers, 1, signal_hidden_size))\n",
        "  signal_prev = torch.zeros(1, signal_emb_size).to(device=dflt_device)\n",
        "  prediction_list     = []\n",
        "  raw_prediction_list = []\n",
        "  cumulative_duration = 0\n",
        "  while cumulative_duration <= float(event_steps-1):\n",
        "    dynamic_idx = int(cumulative_duration)\n",
        "    conditioning = encoded_conditioning[0, dynamic_idx, :].view(1,-1)\n",
        "    signal_input = torch.cat((signal_prev, conditioning), dim=1)\n",
        "    signal_input = signal_input.view(1,1,-1)\n",
        "    s_hat_pre, hidden = signal_forward_pass(signal_input, hidden )\n",
        "    #s_hat = torch.sigmoid(s_hat)  #smoothens the future prob dist\n",
        "    s_hat_pitch  = F.softmax( s_hat_pre[0, 0, 0:rhythm_idx_ini], dim = 0 )\n",
        "    s_hat_rhythm = F.softmax( s_hat_pre[0, 0, rhythm_idx_ini:], dim = 0 )\n",
        "    \n",
        "    raw_s_hat = torch.cat ((s_hat_pitch,s_hat_rhythm) , dim = 0).view(1,-1)\n",
        "    raw_prediction_list.append(raw_s_hat)        \n",
        "    \n",
        "    if sample == False:\n",
        "        note_max , note_argmax = s_hat_pitch.max(0)\n",
        "        rhythm_max , rhythm_argmax = s_hat_rhythm.max(0)\n",
        "        \n",
        "    if sample == True:\n",
        "        note_prob_dist = torch.distributions.Categorical(s_hat_pitch)\n",
        "        note_argmax = int(note_prob_dist.sample())\n",
        "        rhythm_prob_dist = torch.distributions.Categorical(s_hat_rhythm)\n",
        "        rhythm_argmax = int(rhythm_prob_dist.sample())\n",
        "    \n",
        "    s_hat = torch.zeros(1, signal_emb_size)\n",
        "    s_hat[0, int(note_argmax)] = 1  \n",
        "    s_hat[0, int(rhythm_idx_ini + int(rhythm_argmax))] = 1                    \n",
        "    prediction_list.append(s_hat)\n",
        "\n",
        "    #pitch_idx, rhythm_idx = list((s_hat != 0).nonzero())\n",
        "    #pitch_idx, rhythm_idx = int(pitch_idx), int(rhythm_idx)\n",
        "    #duration_type_idx = rhythm_argmax - rhythm_idx_ini\n",
        "    duration_type = durations_list[rhythm_argmax]\n",
        "    cumulative_duration += duration_type\n",
        "\n",
        "    signal_prev  = s_hat.to(device=dflt_device)\n",
        "\n",
        "  prediction = torch.cat(prediction_list)\n",
        "  raw_prediction = torch.cat(raw_prediction_list)\n",
        "\n",
        "  return prediction , raw_prediction\n",
        "\n",
        "e = E[1,:,:]\n",
        "prediction , raw_prediction = net_generate(e, sample=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAFJZ-mw8vH7",
        "colab_type": "code",
        "outputId": "07e6a005-6782-45c4-c4f1-ae9fc29958f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(prediction.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([61, 89])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xrld_D1PrpFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conditioning_prueba = event_forward_pass(E[0,:,:].view(1,16,-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT26iejytLhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conditioning_prueba[0,0,:].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O0LbIBJtYCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_prueba = torch.cat( (S_input.data[0,0:89], conditioning_prueba[0,0,:]), dim=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNqYgQ12uxMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_prueba = input_prueba.view(1,1,105)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxPXX2MYu8D0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prueba_h_ini = (signal_h_ini[0][0,0,:].view(1,1,128), signal_h_ini[1][0,0,:].view(1,1,128))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIv1eTkgvFDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_hat, hidden = signal_forward_pass(input_prueba, prueba_h_ini )\n",
        "s_hat = torch.sigmoid(s_hat)\n",
        "s_hat.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vkNtYLjvN0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_hat = 1*(s_hat>=0.5)\n",
        "s_hat = s_hat.type(torch.FloatTensor)\n",
        "s_hat = s_hat.to(device=dflt_device)\n",
        "print(s_hat.shape)\n",
        "conditioning_prueba[0,0,:].view(1,1,16).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBa_giDcwILr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_next = torch.cat((s_hat, conditioning_prueba[0,0,:].view(1,1,16)), dim=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm7dH_LdwWCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_hat2, hidden = signal_forward_pass(s_next, hidden )\n",
        "s_hat2 = torch.sigmoid(s_hat2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1If9e36wsqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_hat1, hidden = signal_forward_pass(input_prueba, prueba_h_ini )\n",
        "s_hat1 = torch.sigmoid(s_hat1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A58GjRJCwuK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_hat1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}